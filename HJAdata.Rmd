---
title: "HJAdata"
author: "Mallard"
date: "10/24/2022"
output: html_document
---
Outputs of this script are:
- allData: combined Q and met daily data
- allQData: daily Q data from WS1,2,3,10
- met: daily all met variables including precip, temp, vpd (max min mean of the latter two)
- metXXXX: daily individual met variables 
- XXXraw: loaded csv data from HJA data portal. No restructuring or filtering

Chunks at bottom are scratch to do some exploratory visualization and determine correct filtering for met data

Loading packages and input data
```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(zoo)
library(climwin)
library(plotly)
library(imputeTS)
```

Input data catalog:
- HF00402: Flow data, daily runoff in (normalized to area)
- MS00103: Precip data, mm
- HT00401: Temp @ gauges, deg C [per SteveW likely to be lower quality temp enclosures]
- MS00101: Daily (min/max/mean) Temp @ met stations, deg C
- MS00108: vpd (min/max/mean) @ met stations, mb
```{r inputData, echo=FALSE, message=FALSE}
qRaw <- read.csv("Data/HF00402_v12.csv")
precipRaw <- read.csv("Data/MS00103_v8.csv")
tempRaw <- read.csv("Data/MS00101_v9.csv")
vpdRaw <- read.csv("Data/MS00108_v9.csv")

```

Figuring out extents of data at met stations. 
```{r metDataExtent, echo=FALSE, message=FALSE}
metSiteLength <- precipRaw %>% 
  group_by(SITECODE) %>% 
  summarise(
    timeStart = min(DATE),
    timeEnd = max(DATE)
  ) %>% 
  arrange(timeStart)

metSiteLength
```

Per Steve Wondzell PRIMET is the station to use for precip, CS2MET is likely a bit too overgrown and may be undercatching. PRIMET also site to use for vpd and temp per Chris Daly.

Filter and restructure met:
- Only one site
- Combine temp, vpd, and precip
- Filtering:
  - Temp
    - Sensor heights = 1.5 m. (probes 4,5,6)
    - Probe 5 and 6, or when there's no value for 5,6 use probe 4
  - Vpd
    - Sensor height = 1.5 m
    - Probe 5 or probe 4 after 2000-5-29
```{r metData, echo=FALSE, message=FALSE}
metPrecip <- precipRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  select(DATE,PRECIP_TOT_DAY) %>% 
  rename(precip = PRECIP_TOT_DAY) %>% 
  mutate(DATE = as_date(DATE))

metTemp <- tempRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) %>% 
  filter(PROBE_CODE %in% c("AIRPRI06", "AIRPRI05", "AIRPRI04")) %>%
  arrange(DATE, factor(PROBE_CODE, levels = c("AIRPRI06", "AIRPRI05", "AIRPRI04"))) %>% #Sort by probe order here
  drop_na(AIRTEMP_MEAN_DAY) %>% 
  distinct(DATE, .keep_all = TRUE) %>% #Takes the first value for that date, so probe 4 only taken if no 5 or 6
  padr::pad() %>% #Add back NA values
  select(DATE,AIRTEMP_MEAN_DAY,AIRTEMP_MAX_DAY,AIRTEMP_MIN_DAY) %>% 
  rename(meanTemp = AIRTEMP_MEAN_DAY, maxTemp = AIRTEMP_MAX_DAY, minTemp = AIRTEMP_MIN_DAY)

metVpd <- vpdRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) %>% 
  filter(HEIGHT == 150) %>% 
  filter((PROBE_CODE == "VPDPRI05") |
           ((PROBE_CODE == "VPDPRI04") & (DATE > as_date("2000-05-29")))
           ) %>% 
  select(DATE,VPD_MEAN_DAY,VPD_MAX_DAY,VPD_MIN_DAY) %>% 
  rename(meanVpd = VPD_MEAN_DAY, maxVpd = VPD_MAX_DAY, minVpd = VPD_MIN_DAY)
  

#Create a list of these data frames and then use reduce to sequentially do a full join
metList <- list(metPrecip, metTemp, metVpd)

met <- metList %>% 
  reduce(full_join, by = "DATE")

rm(metList) #Cleanup environment
```

Now lets put some datasets together for multiple watersheds (including WS1)
- WSs 1,2,3,10
- Total Q (in inches), rather than mean, max, min
- Convert DATE from character to date format
- Convert inches to mm
- Reformat as a wide table
- Set dates past 2022 back to 20th century (caused by 2 digit dates in raw data)
- Reduce to common days b/n met and q, but not all common days for watersheds
- Add water year column
- Merge with "met" variable from "metData" chunk to record length of met data
  - This adds precip, temp (mean, max, min), and vpd (mean, max, min)
```{r allWSdata}
#Q data
allQData <- qRaw %>% 
  filter(SITECODE %in% c("GSWS01","GSWS02","GSWS03","GSWS10")) %>% 
  select(DATE,TOTAL_Q_AREA,SITECODE) %>% 
  rename(runoff = TOTAL_Q_AREA) %>% 
  mutate(DATE = mdy(DATE)) %>% 
  mutate(runoff = runoff * 25.4) %>% 
  pivot_wider(names_from = SITECODE,values_from = runoff) %>% 
  rename(WS1 = GSWS01,
         WS2 = GSWS02,
         WS3 = GSWS03,
         WS10 = GSWS10) %>% 
  mutate(DATE = if_else(year(DATE)>2022,DATE-years(100),DATE)) #ymd function has trouble with 2 digit years
  
#Merge with met data, add WY column
allData <- full_join(met, allQData, by='DATE') %>%
  mutate(yr = year(DATE), mn = month(DATE)) %>%
  mutate(WY = ifelse(mn > 9, yr + 1, yr)) %>%
  select(-c(yr,mn))
```

Working on gap filling vpd.
- Largest gaps are:
  - 94-9-9:95-8-17
  - 98-5-24:98-12-12
  - These occur when only one vpd sensor is available
- There is temp data at primet that could be used for a regression gap filling
- Still need to check 15min vpd data to see if there's anything there
```{r VPD_Gaps}
#Data as it's currently filtered
vpdTS <- vpdRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) %>% 
  filter(HEIGHT == 150) %>% 
  filter((PROBE_CODE == "VPDPRI05") |
           ((PROBE_CODE == "VPDPRI04") & (DATE > as_date("2000-05-29")))
           )

#Continuity of sensors
sensorExtent <- ggplot(data = vpdTS) +
  geom_point(aes(x = DATE, y = as.factor(PROBE_CODE)))

#Missing day locations
imputeTS::ggplot_na_distribution(x = vpdTS$VPD_MEAN_DAY, x_axis_labels = vpdTS$DATE)
imputeTS::ggplot_na_gapsize(x = vpdTS$VPD_MEAN_DAY)

imputeTS::statsNA(vpdTS$VPD_MEAN_DAY)

vpdPlot <- ggplot(data = vpdTS) +
  geom_point(aes(x = DATE, y = VPD_MEAN_DAY))
# ggplotly(vpdPlot)

#Check if temp could be used to gapfill vpd
tempTS <- tempRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) %>% 
  filter(HEIGHT == 150) %>% 
  filter(!(PROBE_CODE == "AIRPRI03")) %>% 
  filter(PROBE_CODE %in% c("AIRPRI06", "AIRPRI05") | 
           (PROBE_CODE == "AIRPRI04" & between(DATE, as.Date("2004-6-15"), as.Date("2004-6-25"))))

imputeTS::ggplot_na_distribution(x = tempTS$AIRTEMP_MEAN_DAY, x_axis_labels = tempTS$DATE)

imputeTS::statsNA(tempTS$AIRTEMP_MEAN_DAY)

tempPlot <- ggplot(data = tempTS) +
  geom_point(aes(x = DATE, y = AIRTEMP_MEAN_DAY))
ggplotly(tempPlot)

#Which NAs of vpd are covered by temp?
vpdTempOverlap <- vpdTS %>% 
  left_join(., tempTS, by = "DATE") %>% 
  mutate(overlapTemp = case_when(!is.na(VPD_MEAN_DAY) ~ 2,
                                 is.na(VPD_MEAN_DAY) & !is.na(AIRTEMP_MEAN_DAY) ~ 1,
                                 TRUE ~ NA_real_))

overlapPlot <- ggplot(data = vpdTempOverlap) +
  geom_point(aes(x = DATE, y = overlapTemp))
overlapPlot

ggplot_na_distribution(vpdTempOverlap$overlapTemp, x_axis_labels = vpdTempOverlap$DATE)
statsNA(vpdTempOverlap$overlapTemp)
statsNA(vpdTempOverlap$VPD_MEAN_DAY)

ggplot_na_distribution(vpdTempOverlap$AIRTEMP_MEAN_DAY, x_axis_labels = vpdTempOverlap$DATE)
ggplot_na_distribution(vpdTempOverlap$VPD_MEAN_DAY, x_axis_labels = vpdTempOverlap$DATE)

#Other temp values at PRIMET to use? Remove height and sensor filtering from temp
tempTS2 <- tempRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE))
  # filter(between(DATE, as.Date("1998-01-01"), as.Date("1999-12-31"))) %>% 
  # filter(PROBE_CODE == "AIRPRI05")
  
# ggplotly(
ggplot(data = tempTS2) +
  geom_point(aes(x = DATE, y = as.factor(PROBE_CODE)))
# )

ggplot(data = tempTS2, aes(DATE, AIRTEMP_MEAN_DAY)) +
  geom_point(color = "steelblue", size = 1) +
  facet_wrap(~ PROBE_CODE)
  
#Re-do temp sensor filtering above so that sensors are taken in order: 
#6, 5, 4
tempTS3 <- tempRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) %>% 
  filter(PROBE_CODE %in% c("AIRPRI06", "AIRPRI05", "AIRPRI04")) %>%
  arrange(DATE, factor(PROBE_CODE, levels = c("AIRPRI06", "AIRPRI05", "AIRPRI04"))) %>% 
  drop_na(AIRTEMP_MEAN_DAY) %>% 
  distinct(DATE, .keep_all = TRUE) %>% 
  padr::pad() %>% 
  select(DATE,AIRTEMP_MEAN_DAY,AIRTEMP_MAX_DAY,AIRTEMP_MIN_DAY) %>% 
  rename(meanTemp = AIRTEMP_MEAN_DAY, maxTemp = AIRTEMP_MAX_DAY, minTemp = AIRTEMP_MIN_DAY)



ggplot_na_distribution(tempTS3$meanTemp)
statsNA(tempTS3$meanTemp)

#See if similar filtering as above with temp could be done with vpd to improve it
vpdTS2 <- vpdRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) 
  # filter(PROBE_CODE == "VPDPRI01")
  
  # filter(HEIGHT == 150) %>% 
  # filter((PROBE_CODE == "VPDPRI05") |
  #          ((PROBE_CODE == "VPDPRI04") & (DATE > as_date("2000-05-29")))
           # )

ggplot_na_distribution(vpdTS2$VPD_MEAN_DAY, x_axis_labels = vpdTS2$DATE)

ggplot(data = vpdTS2, aes(DATE, VPD_MEAN_DAY)) +
  geom_point(color = "steelblue", size = 1) +
  facet_wrap(~ PROBE_CODE)

```


Troubleshooting of primet data to explain odd figures in previous chunk
```{r PRIMET_troubleshooting}
#First, let's check for duplicate data in allData data frame
# tsAllData <- allData %>% 
#   mutate(tsYear = year(DATE)) %>% 
#   count(tsYear)

#Starting in 1994 amount of data increased by almost 10 fold


#Check some of the variables for multiple options
unique(tsRawPrecipData$PRECIP_METHOD)
unique(tsRawPrecipData$PROBE_CODE)

#Temp
tsRawTempData <- tempRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE))

#Check some of the variables for multiple options
unique(tsRawTempData$AIRTEMP_METHOD)
unique(tsRawTempData$PROBE_CODE)

#Multiple values for all of these. Check in raw data where duplicate days coming from
#Filter to 1994 in both data
tsRawPrecipDataSub <- tsRawPrecipData %>% 
  filter(between(DATE, as.Date('1994-01-01'), as.Date('1994-12-31')))

tsRawTempDataSub <- tsRawTempData %>% 
  filter(between(DATE, as.Date('1980-01-01'), as.Date('1980-12-31')))


#For temp, what heights are available through the record
tempHeights <- tsRawTempData %>% 
  select(HEIGHT,DATE) %>% 
  mutate(myYears = year(DATE)) %>% 
  filter(HEIGHT == 150) %>% 
  group_by(myYears) %>% 
  count(HEIGHT)


unique(tempHeights$HEIGHT)

#Ok, so filtering temp data to 150 m only. Now figure out what other unique values
# tsRawTempData <- tempRaw %>% 
#   filter(SITECODE == "PRIMET") %>% 
#   mutate(DATE = as_date(DATE)) %>% 
#   filter(HEIGHT == 150)
# 
# unique(tsRawTempData$AIRTEMP_METHOD)
# unique(tsRawTempData$PROBE_CODE)
# 
# sensorExtent <- ggplot(data = tsRawTempData) + 
#   geom_point(aes(x = DATE, y = as.factor(PROBE_CODE)))
# 
# sensorExtent
# 
# ggplotly(sensorExtent)

#Filtering additionally to sensors 5,6, and a few days of 4
tsRawTempData <- tempRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) %>% 
  filter(HEIGHT == 150) %>% 
  filter(!(PROBE_CODE == "AIRPRI03")) %>% 
  filter(PROBE_CODE %in% c("AIRPRI06", "AIRPRI05") | 
           (PROBE_CODE == "AIRPRI04" & between(DATE, as.Date("2004-6-15"), as.Date("2004-6-25"))))

sensorExtent <- ggplot(data = tsRawTempData) + 
  geom_point(aes(x = DATE, y = as.factor(PROBE_CODE)))

sensorExtent

ggplotly(sensorExtent)

#Filtering to methods
unique(tsRawTempData$AIRTEMP_METHOD)
unique(tsRawTempData$PROBE_CODE)

#Method time extents
sensorExtent <- ggplot(data = tsRawTempData) +
  geom_point(aes(x = DATE, y = as.factor(AIRTEMP_METHOD)))

sensorExtent

ggplotly(sensorExtent)

#Similar work on filtering precip data
tsRawPrecipData <- precipRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE))

unique(tsRawPrecipData$PROBE_CODE)
unique(tsRawPrecipData$PRECIP_METHOD)

sensorExtent <- ggplot(data = tsRawPrecipData) +
  geom_point(aes(x = DATE, y = as.factor(PRECIP_METHOD)))

ggplotly(sensorExtent)

nrow(tsRawPrecipData)
length(unique(tsRawPrecipData$DATE))

#Same filtering with vpd data
tsRawVpdData <- vpdRaw %>% 
  filter(SITECODE == "PRIMET") %>% 
  mutate(DATE = as_date(DATE)) %>% 
  filter(HEIGHT == 150) %>% 
  filter((PROBE_CODE == "VPDPRI05") |
           ((PROBE_CODE == "VPDPRI04") & (DATE > as_date("2000-05-29")))
           )

unique(tsRawVpdData$PROBE_CODE)
unique(tsRawVpdData$VPD_METHOD)
unique(tsRawVpdData$HEIGHT)

sensorExtent <- ggplot(data = tsRawVpdData) +
  geom_point(aes(x = DATE, y = as.factor(PROBE_CODE)))

ggplotly(sensorExtent)

#Still to-do:
#Sort out flags on data to remove low-quality measurements
#Look through discharge data to do the same

#Check where variables exist for WS2Data
imputeTS::ggplot_na_distribution(x = WS2data$meanVpd, x_axis_labels = WS2data$DATE)
imputeTS::ggplot_na_distribution(x = WS2data$meanTemp, x_axis_labels = WS2data$DATE)
```

